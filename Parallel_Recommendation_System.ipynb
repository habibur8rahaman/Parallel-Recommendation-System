{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM57NDCT1mIePucPbA5Izaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/habibur8rahaman/Parallel-Recommendation-System/blob/main/Parallel_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://www.kaggle.com/datasets/somnambwl/bookcrossing-dataset?select=Users.csv\n",
        "\n",
        "Only need the Books and Ratings files. Before loading the datasets:\n",
        "1. Replace ',' with ''\n",
        "2. Then Replace ';' with ','"
      ],
      "metadata": {
        "id": "QivHdMeAh2zW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A Simple Python Solution**\n",
        "\n",
        "This was part of our initial work, and is not included in the project report. However, its useful to demonstrate how large datasets can impact the computational resource even when using a simple algorithm (K-Nearest Neighbours)"
      ],
      "metadata": {
        "id": "wSlOAHNUwdG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###=================Recommended system using simple python approach===========\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "\n",
        "books = pd.read_csv(\"/content/Books.csv\").sample(frac=0.20)\n",
        "ratings = pd.read_csv(\"/content/Ratings.csv\").sample(frac=0.20)\n",
        "\n",
        "# merging ratings with books\n",
        "full_data = pd.merge(ratings, books, on='ISBN')\n",
        "\n",
        "# create user-book-rating matrix\n",
        "user_ratings = full_data.pivot_table(index='User-ID', columns='ISBN', values='Rating').fillna(0)\n",
        "matrix = csr_matrix(user_ratings.values)\n",
        "\n",
        "# KNN model\n",
        "model_knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
        "model_knn.fit(matrix)\n",
        "\n",
        "\n",
        "\n",
        "def recommend_books_collab(user_id, n_recommendations=5):\n",
        "\n",
        "      # finding similar users\n",
        "      user_index = user_ratings.index.get_loc(user_id)\n",
        "      distances, indices = model_knn.kneighbors(\n",
        "          user_ratings.iloc[user_index, :].values.reshape(1, -1),\n",
        "          n_neighbors=11  # similar users + self\n",
        "      )\n",
        "\n",
        "      # books liked by similar users\n",
        "      similar_users = user_ratings.index[indices.flatten()[1:]]\n",
        "      similar_users_ratings = user_ratings.loc[similar_users]\n",
        "\n",
        "      # books the user has already rated\n",
        "      user_rated = set(full_data[full_data['User-ID'] == user_id]['ISBN'])\n",
        "\n",
        "      # average ratings from similar users\n",
        "      avg_ratings = similar_users_ratings.mean(axis=0)\n",
        "\n",
        "      # Filtering out books already rated and get top recommendations\n",
        "      recommendations = avg_ratings[~avg_ratings.index.isin(user_rated)]\n",
        "      recommendations = recommendations.sort_values(ascending=False).head(n_recommendations)\n",
        "\n",
        "      #recommended book details\n",
        "      recommended_books = books[books['ISBN'].isin(recommendations.index)]\n",
        "\n",
        "      return recommended_books\n",
        "\n",
        "\n",
        "\n",
        "### Example usage:\n",
        "\n",
        "# a user that exists in the ratings data\n",
        "existing_users = ratings['User-ID'].unique()\n",
        "\n",
        "# recommendations\n",
        "if len(existing_users) > 0:\n",
        "\n",
        "    user_id = existing_users[0]\n",
        "\n",
        "    print(f\"Recommendations for user {user_id}:\")\n",
        "    recommendations = recommend_books_collab(user_id)\n",
        "    if isinstance(recommendations, pd.DataFrame):\n",
        "        print(recommendations[['Title', 'Author']])\n",
        "    else:\n",
        "        print(recommendations)"
      ],
      "metadata": {
        "id": "pe7_f1SgCVVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba8b922-fd94-4875-9559-35b4d70d2829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommendations for user 138844:\n",
            "                                   Title                Author\n",
            "244117    The Clue in the Crumbling Wall         Carolyn Keene\n",
            "182796  The Revenge of Murray the Mantis      Maureen Spurgeon\n",
            "1                           Clara Callan  Richard Bruce Wright\n",
            "179431              The Underground City           Jules Verne\n",
            "107835               The Moon of Gomrath           Alan Garner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alternating Least Squares Method from PySpark**\n",
        "\n",
        "This code block uses the ALS method from MLlib to train on the dataset."
      ],
      "metadata": {
        "id": "a9dJVIsR8cRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###==========================using ALS(Alternating Least Squares) model from Spark=========================\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.functions import col, explode, udf, lit\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "\n",
        "#Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BookRecommendationSystem\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "books_df = spark.read.csv(\"/content/Books.csv\", header=True, inferSchema=True)\n",
        "ratings_df = spark.read.csv(\"/content/Ratings.csv\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "#removing nulls\n",
        "ratings_df = ratings_df.filter((ratings_df['Rating'] > 0) &\n",
        "                              (ratings_df['Rating'] <= 10)).dropna()\n",
        "books_df = books_df.dropna()\n",
        "\n",
        "#keeping the ratings for books that exist in Books.csv\n",
        "ratings_df = ratings_df.join(books_df.select(\"ISBN\"), \"ISBN\", \"inner\")\n",
        "\n",
        "# index User-ID\n",
        "user_indexer = StringIndexer(inputCol=\"User-ID\", outputCol=\"userIndex\")\n",
        "user_model = user_indexer.fit(ratings_df)\n",
        "ratings_df = user_model.transform(ratings_df)\n",
        "\n",
        "\n",
        "#combining all ISBNs\n",
        "all_isbns = ratings_df.select(\"ISBN\").union(books_df.select(\"ISBN\")).distinct()\n",
        "\n",
        "# Fitting the indexer on all ISBN\n",
        "isbn_indexer = StringIndexer(inputCol=\"ISBN\", outputCol=\"itemIndex\")\n",
        "isbn_model = isbn_indexer.fit(all_isbns)\n",
        "\n",
        "\n",
        "# consistent indexer (Otherwise was getting NULLs in result)\n",
        "ratings_df = isbn_model.transform(ratings_df)\n",
        "books_df = isbn_model.transform(books_df)\n",
        "\n",
        "#ALS model\n",
        "als = ALS(\n",
        "    maxIter=15,\n",
        "    regParam=0.05,\n",
        "    rank=20,\n",
        "    userCol=\"userIndex\",\n",
        "    itemCol=\"itemIndex\",\n",
        "    ratingCol=\"Rating\",\n",
        "    coldStartStrategy=\"drop\",\n",
        "    nonnegative=True,\n",
        "    implicitPrefs=False\n",
        ")\n",
        "\n",
        "# 20% of data for evaluation\n",
        "train, test = ratings_df.randomSplit([0.8, 0.2])\n",
        "model = als.fit(train)"
      ],
      "metadata": {
        "id": "qle-rUa1QODY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Serialized Lookup Approach**\n",
        "\n",
        "Dot Product Computaiton in Serial Manner. \\\\\n",
        "in the output, the predicted rating column is actually the dot product to find similarity. So the higher values means the user will most probably like it more. Thats why its value is different than the usual 0-10 rating scale."
      ],
      "metadata": {
        "id": "LfGPyTZz87nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###===============================Serialized Lookup======\n",
        "\n",
        "\n",
        "from time import time\n",
        "\n",
        "\n",
        "#recommendation function\n",
        "def recommend_books_for_user(user_id, model, ratings_df, books_df):\n",
        "\n",
        "    start_tiem = time()\n",
        "\n",
        "    #userIndex finding\n",
        "    user_index_lookup = ratings_df.select(\"User-ID\", \"userIndex\").distinct()\n",
        "    target_user_index_df = user_index_lookup.filter(col(\"User-ID\") == user_id)\n",
        "\n",
        "    try:\n",
        "        target_user_index = target_user_index_df.select(\"userIndex\").first()[0]\n",
        "    except:\n",
        "        print(f\"User ID {user_id} not found or has no ratings.\")\n",
        "        return\n",
        "\n",
        "    #user DataFrame\n",
        "    user_df = spark.createDataFrame([(target_user_index,)], [\"userIndex\"])\n",
        "\n",
        "    # recommendations\n",
        "    user_recs = model.recommendForUserSubset(user_df, 10)\n",
        "\n",
        "    if user_recs.count() == 0:\n",
        "        print(f\"No recommendations available for user {user_id}\")\n",
        "        return\n",
        "\n",
        "    recs = user_recs.select(\"userIndex\", explode(\"recommendations\").alias(\"rec\")) \\\n",
        "                    .select(\"userIndex\", col(\"rec.itemIndex\").alias(\"itemIndex\"),\n",
        "                            col(\"rec.rating\").alias(\"predictedRating\"))\n",
        "\n",
        "    # Join with books\n",
        "    books_info = books_df.select(\"itemIndex\", \"ISBN\", \"Title\", \"Author\", \"Publisher\") \\\n",
        "                         .dropDuplicates([\"itemIndex\"])\n",
        "\n",
        "    recommended_books = recs.join(books_info, on=\"itemIndex\", how=\"inner\") \\\n",
        "                           .filter(col(\"Title\").isNotNull()) \\\n",
        "                           .orderBy(col(\"predictedRating\").desc())\n",
        "\n",
        "    recommended_books.select(\"Title\", \"Author\", \"Publisher\",\n",
        "                               \"predictedRating\").show(truncate=False, n=10)\n",
        "\n",
        "    end_time = time()\n",
        "    print(f\"Time taken: {end_time - start_tiem} seconds\")\n",
        "\n",
        "\n",
        "### Recommendation for entered user:\n",
        "recommend_books_for_user(\"175070\", model, ratings_df, books_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsOm6PMJQSlA",
        "outputId": "8b645a5a-2ec0-4ae2-8556-3f30718214d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------+-----------------------+--------------------+---------------+\n",
            "|Title                                                               |Author                 |Publisher           |predictedRating|\n",
            "+--------------------------------------------------------------------+-----------------------+--------------------+---------------+\n",
            "|Sacred Honor                                                        |Lillian Cauldwell      |Publishamerica      |15.001067      |\n",
            "|Sticks & stones &amp, ice cream cones,: The craft book for children |Phyllis Fiarotta       |Workman Pub. Co     |14.970831      |\n",
            "|Hitchhiking Vietnam                                                 |Karin Muller           |Globe Pequot        |14.352041      |\n",
            "|And Then There Were None                                            |Agatha Christie        |St. Martin's Griffin|14.111767      |\n",
            "|Fried Butter: A Food Memoir                                         |Abe Opincar            |Soho Press          |13.891222      |\n",
            "|Mercy                                                               |Caroline B. Cooney     |Pan Macmillan       |13.84239       |\n",
            "|Nina: Adolescence                                                   |Amy Hassinger          |Listen & Live Audio |13.705235      |\n",
            "|Use What You've Got and Other Business Lessons I Learned from My Mom|Barbara Corcoran       |Listen & Live Audio |13.705235      |\n",
            "|Rimas y leyendas (ClÃ¡sicos Fraile , 3)                             |Gustavo Adolfo BÃ©cquer|Ediciones Fraile    |13.575473      |\n",
            "|The BetweenTime                                                     |Barry Gerdsen          |Xlibris Corporation |13.454098      |\n",
            "+--------------------------------------------------------------------+-----------------------+--------------------+---------------+\n",
            "\n",
            "Time taken: 57.08277082443237 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parallelized Lookup**\n",
        "\n",
        "Splits the lookup tasks amid partitions to parallelly look for similarity and generate recommendations \\\\\n",
        "\n",
        "This run of code is done with 100 partitions, that is why its showing more time than serial computation. However, with smaller partition the lookup time is much faster than serial ALS. Change the partition value in <item_scores = model.itemFactors.repartition(100, \"id\")> line from 100 to any other to see changes in computation time. An detailed comparition of complettion time for different partiton values is given in the report"
      ],
      "metadata": {
        "id": "e9XzSRz59QfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###=============Lookups in parallel across multiple nodes==================\n",
        "\n",
        "\n",
        "from math import e\n",
        "import numpy as np\n",
        "\n",
        "#dot product UDF for parallel computation\n",
        "dot_prod_udf = udf(lambda user_vec, item_vec: float(np.dot(user_vec, item_vec)), FloatType())\n",
        "\n",
        "def recommend_books_for_user(user_id, model, ratings_df, books_info):\n",
        "\n",
        "    start_time = time()\n",
        "\n",
        "    # converts user_id to index\n",
        "    user_index_lookup = ratings_df.select(\"User-ID\", \"userIndex\").distinct()\n",
        "    target_user_index_df = user_index_lookup.filter(col(\"User-ID\") == user_id)\n",
        "\n",
        "    try:\n",
        "        target_user_index = target_user_index_df.select(\"userIndex\").first()[0]\n",
        "    except:\n",
        "        print(f\"User ID {user_id} not found or has no ratings.\")\n",
        "        return\n",
        "\n",
        "    # getting user factor vector\n",
        "    user_factors = model.userFactors\n",
        "    user_vector = user_factors.filter(col(\"id\") == target_user_index) \\\n",
        "                            .select(\"features\").first()[0]\n",
        "\n",
        "    # Broadcasts user vector to all workers\n",
        "    bc_user_vector = spark.sparkContext.broadcast(user_vector)\n",
        "\n",
        "    # parallel dot product computation\n",
        "    item_scores = model.itemFactors.repartition(100, \"id\") \\\n",
        "        .withColumn(\"score\", dot_prod_udf(col(\"features\"), lit(bc_user_vector.value))) \\\n",
        "        .orderBy(col(\"score\").desc()).limit(10)\n",
        "\n",
        "    # joining with book info and showing results\n",
        "    recommendations = item_scores.join(\n",
        "        books_info,\n",
        "        item_scores[\"id\"] == books_info[\"itemIndex\"],\n",
        "        \"inner\"\n",
        "    ).select(\n",
        "        \"Title\", \"Author\", \"Publisher\", \"score\"\n",
        "    ).orderBy(col(\"score\").desc())\n",
        "\n",
        "    recommendations.show(truncate=False, n=10)\n",
        "\n",
        "    end_time = time()\n",
        "    print(f\"Time taken: {end_time - start_time} seconds\")\n",
        "\n",
        "    # Clean up\n",
        "    bc_user_vector.unpersist()\n",
        "\n",
        "# Recommendation for entered user\n",
        "recommend_books_for_user(\"175070\", model, ratings_df, books_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6qXE4KtFWhp",
        "outputId": "54e30306-3df7-44d7-f31e-cbe4619344ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------+-----------------------+--------------------+----------+\n",
            "|Title                                                               |Author                 |Publisher           |score     |\n",
            "+--------------------------------------------------------------------+-----------------------+--------------------+----------+\n",
            "|Sacred Honor                                                        |Lillian Cauldwell      |Publishamerica      |15.001067 |\n",
            "|Sticks & stones &amp, ice cream cones,: The craft book for children |Phyllis Fiarotta       |Workman Pub. Co     |14.970831 |\n",
            "|Hitchhiking Vietnam                                                 |Karin Muller           |Globe Pequot        |14.352042 |\n",
            "|And Then There Were None                                            |Agatha Christie        |St. Martin's Griffin|14.111768 |\n",
            "|Fried Butter: A Food Memoir                                         |Abe Opincar            |Soho Press          |13.891222 |\n",
            "|Mercy                                                               |Caroline B. Cooney     |Pan Macmillan       |13.842391 |\n",
            "|Use What You've Got and Other Business Lessons I Learned from My Mom|Barbara Corcoran       |Listen & Live Audio |13.7052355|\n",
            "|Nina: Adolescence                                                   |Amy Hassinger          |Listen & Live Audio |13.7052355|\n",
            "|Rimas y leyendas (ClÃ¡sicos Fraile , 3)                             |Gustavo Adolfo BÃ©cquer|Ediciones Fraile    |13.575473 |\n",
            "|The BetweenTime                                                     |Barry Gerdsen          |Xlibris Corporation |13.454097 |\n",
            "+--------------------------------------------------------------------+-----------------------+--------------------+----------+\n",
            "\n",
            "Time taken: 83.06365299224854 seconds\n"
          ]
        }
      ]
    }
  ]
}